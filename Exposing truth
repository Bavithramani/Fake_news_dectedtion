import pandas as pd
import numpy as np
import re
import nltk
import matplotlib.pyplot as plt
import seaborn as sns

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Ensure necessary downloads
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# 1. Load Data
def load_data():
    fake = pd.read_csv('Fake.csv')
    true = pd.read_csv('True.csv')

    fake['label'] = 0
    true['label'] = 1

    data = pd.concat([fake, true], ignore_index=True)
    data = data[['title', 'text', 'label']]
    data.dropna(inplace=True)

    return data

# 2. Clean Text
def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)
    text = re.sub(r'\@w+|\#','', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    cleaned_tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]
    return ' '.join(cleaned_tokens)

# 3. Preprocess
def preprocess(data):
    data['text'] = data['text'].apply(clean_text)
    return data

# 4. Feature Extraction
def extract_features(data):
    vectorizer = TfidfVectorizer(max_features=5000)
    X = vectorizer.fit_transform(data['text'])
    y = data['label']
    return X, y, vectorizer

# 5. Train Models
def train_models(X_train, y_train):
    models = {
        'Logistic Regression': LogisticRegression(max_iter=1000),
        'Naive Bayes': MultinomialNB(),
        'Random Forest': RandomForestClassifier()
    }

    for name, model in models.items():
        model.fit(X_train, y_train)
    return models

# 6. Evaluate Models
def evaluate_models(models, X_test, y_test):
    for name, model in models.items():
        print(f"\n{name} Results:")
        y_pred = model.predict(X_test)
        print(confusion_matrix(y_test, y_pred))
        print(classification_report(y_test, y_pred))

# 7. Predict Custom Input
def predict_news(model, vectorizer, input_text, threshold=0.8):
    cleaned = clean_text(input_text)
    vector = vectorizer.transform([cleaned])
    prob = model.predict_proba(vector)[0][1]  # Probability of REAL

    print(f"\nInput: {input_text}")
    print(f"Probability of REAL: {prob:.2f}")
    if prob > threshold:
        print("Prediction: REAL")
    else:
        print("Prediction: FAKE")

# 8. Main
def main():
    data = load_data()
    data = preprocess(data)
    X, y, vectorizer = extract_features(data)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    models = train_models(X_train, y_train)
    evaluate_models(models, X_test, y_test)

    # Use the best model for prediction (e.g., Logistic Regression)
    while True:
        user_input = input("\nEnter a news article (or type 'exit' to quit):\n")
        if user_input.lower() == 'exit':
            break
        predict_news(models['Logistic Regression'], vectorizer, user_input)
